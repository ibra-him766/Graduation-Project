{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "from gensim import models\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import collections\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"trining set.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>name</th>\n",
       "      <th>text</th>\n",
       "      <th>Stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Ù…Ø´Ø§Ø¹Ù„ #Ø§Ù„Ø²Ø¹ÙŠÙ…_Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠ_Ø§Ù„Ù…Ù„ÙƒÙŠ</td>\n",
       "      <td>Ø¥Ø¹Ù„Ø§Ù† ÙˆØ§Ø·ÙŠ Ù…Ù† Ø³ÙƒØ´Ù†-Ø¨ÙŠ ğŸ˜ğŸ˜‘ğŸ˜ https://t.co/uKArk6lg5k</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Abureem</td>\n",
       "      <td>@shrimpshacksa Ù„Ù„Ø£Ø³Ù ÙƒØ§Ù†Øª ØªØ¬Ø±Ø¨Ø© Ø³ÙŠØ¦Ø© Ù…Ø¹ÙƒÙ… Ø§Ù…Ø³ ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ÙBo riyad ... â™š</td>\n",
       "      <td>RT @Mur_Hus: @Saad_Maestro @MaestroPizzaKSA @l...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Ø¥Ø¨Ø±Ø§Ù‡ÙŠÙ… Ø§Ù„Ù…Ù„Ø­Ù…</td>\n",
       "      <td>@SubwaySaudi Ø¢Ø®Ø± Ù…Ø±Ø© Ø¨Ø¥Ø°Ù† Ø§Ù„Ù„Ù‡ Ø§Ø´ØªØ±ÙŠ Ù…Ù†ÙƒÙ… ØŒ ÙƒÙ„...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Different</td>\n",
       "      <td>Ø§Ù„ÙŠÙˆÙ… Ø¨Ø±ÙˆØ­ Ø³ÙƒØ´Ù† Ø¨ÙŠ Ù…Ù†Ù‚Ø¨Ø© Ø¹Ù†Ø§Ø¯</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                          name  \\\n",
       "0           0  Ù…Ø´Ø§Ø¹Ù„ #Ø§Ù„Ø²Ø¹ÙŠÙ…_Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠ_Ø§Ù„Ù…Ù„ÙƒÙŠ   \n",
       "1           1                       Abureem   \n",
       "2           2               ÙBo riyad ... â™š   \n",
       "3           3                Ø¥Ø¨Ø±Ø§Ù‡ÙŠÙ… Ø§Ù„Ù…Ù„Ø­Ù…   \n",
       "4           4                     Different   \n",
       "\n",
       "                                                text Stance  \n",
       "0  Ø¥Ø¹Ù„Ø§Ù† ÙˆØ§Ø·ÙŠ Ù…Ù† Ø³ÙƒØ´Ù†-Ø¨ÙŠ ğŸ˜ğŸ˜‘ğŸ˜ https://t.co/uKArk6lg5k    neg  \n",
       "1  @shrimpshacksa Ù„Ù„Ø£Ø³Ù ÙƒØ§Ù†Øª ØªØ¬Ø±Ø¨Ø© Ø³ÙŠØ¦Ø© Ù…Ø¹ÙƒÙ… Ø§Ù…Ø³ ...    neg  \n",
       "2  RT @Mur_Hus: @Saad_Maestro @MaestroPizzaKSA @l...    neg  \n",
       "3  @SubwaySaudi Ø¢Ø®Ø± Ù…Ø±Ø© Ø¨Ø¥Ø°Ù† Ø§Ù„Ù„Ù‡ Ø§Ø´ØªØ±ÙŠ Ù…Ù†ÙƒÙ… ØŒ ÙƒÙ„...    neg  \n",
       "4                      Ø§Ù„ÙŠÙˆÙ… Ø¨Ø±ÙˆØ­ Ø³ÙƒØ´Ù† Ø¨ÙŠ Ù…Ù†Ù‚Ø¨Ø© Ø¹Ù†Ø§Ø¯    neg  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data=data[['name','text','Stance']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>text</th>\n",
       "      <th>Stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ù…Ø´Ø§Ø¹Ù„ #Ø§Ù„Ø²Ø¹ÙŠÙ…_Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠ_Ø§Ù„Ù…Ù„ÙƒÙŠ</td>\n",
       "      <td>Ø¥Ø¹Ù„Ø§Ù† ÙˆØ§Ø·ÙŠ Ù…Ù† Ø³ÙƒØ´Ù†-Ø¨ÙŠ ğŸ˜ğŸ˜‘ğŸ˜ https://t.co/uKArk6lg5k</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abureem</td>\n",
       "      <td>@shrimpshacksa Ù„Ù„Ø£Ø³Ù ÙƒØ§Ù†Øª ØªØ¬Ø±Ø¨Ø© Ø³ÙŠØ¦Ø© Ù…Ø¹ÙƒÙ… Ø§Ù…Ø³ ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ÙBo riyad ... â™š</td>\n",
       "      <td>RT @Mur_Hus: @Saad_Maestro @MaestroPizzaKSA @l...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ø¥Ø¨Ø±Ø§Ù‡ÙŠÙ… Ø§Ù„Ù…Ù„Ø­Ù…</td>\n",
       "      <td>@SubwaySaudi Ø¢Ø®Ø± Ù…Ø±Ø© Ø¨Ø¥Ø°Ù† Ø§Ù„Ù„Ù‡ Ø§Ø´ØªØ±ÙŠ Ù…Ù†ÙƒÙ… ØŒ ÙƒÙ„...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Different</td>\n",
       "      <td>Ø§Ù„ÙŠÙˆÙ… Ø¨Ø±ÙˆØ­ Ø³ÙƒØ´Ù† Ø¨ÙŠ Ù…Ù†Ù‚Ø¨Ø© Ø¹Ù†Ø§Ø¯</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3738</th>\n",
       "      <td>adel 22</td>\n",
       "      <td>Ø¬Ù…ÙŠÙ„ Ø§Ù„Ù…Ø·Ø¹Ù… Ø¨ÙŠ Ø§Ù„Ø²Ø­Ù…Ø© ØªÙƒØ±Ù‡Ù†ÙŠ ÙÙŠÙ‡ .. ÙˆØ§Ù„Ù„Ù‡ ØªØ¹Ù‚Ø¯...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3739</th>\n",
       "      <td>Fares Aljumaid</td>\n",
       "      <td>Ø§Ù„Ø´Ø§ÙˆØ±Ù…Ø§ â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸Ø§Ù„ÙƒØ¨Ø§Ø¨ â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸Ø£ÙˆØµØ§Ù„ Ø§Ù„Ø¯Ø¬...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3740</th>\n",
       "      <td>M</td>\n",
       "      <td>Ù„Ø§ØªÙˆØ¬Ø¯ Ù‚Ù‡ÙˆÙ‡ Ø¶Ù…Ù† Ø§Ù„Ø§ÙØ·Ø§Ø± ! ÙÙ‚Ø· Ø¹ØµÙŠØ±Ø§Øª Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø³...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3741</th>\n",
       "      <td>shahd mohamad</td>\n",
       "      <td>Ø§Ù„Ø´ÙˆØ±Ù…Ø§ Ùˆ ÙØ·ÙŠØ±Ø© Ø§Ù„Ø¬Ø¨Ù† ğŸ‘ŒğŸ¼</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3742</th>\n",
       "      <td>Rayyan Alwazzan</td>\n",
       "      <td>The #shawarma, juices and fatayer         Ù„Ø§ ÙŠ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3743 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              name  \\\n",
       "0     Ù…Ø´Ø§Ø¹Ù„ #Ø§Ù„Ø²Ø¹ÙŠÙ…_Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠ_Ø§Ù„Ù…Ù„ÙƒÙŠ   \n",
       "1                          Abureem   \n",
       "2                  ÙBo riyad ... â™š   \n",
       "3                   Ø¥Ø¨Ø±Ø§Ù‡ÙŠÙ… Ø§Ù„Ù…Ù„Ø­Ù…   \n",
       "4                        Different   \n",
       "...                            ...   \n",
       "3738                       adel 22   \n",
       "3739                Fares Aljumaid   \n",
       "3740                             M   \n",
       "3741                 shahd mohamad   \n",
       "3742               Rayyan Alwazzan   \n",
       "\n",
       "                                                   text Stance  \n",
       "0     Ø¥Ø¹Ù„Ø§Ù† ÙˆØ§Ø·ÙŠ Ù…Ù† Ø³ÙƒØ´Ù†-Ø¨ÙŠ ğŸ˜ğŸ˜‘ğŸ˜ https://t.co/uKArk6lg5k    neg  \n",
       "1     @shrimpshacksa Ù„Ù„Ø£Ø³Ù ÙƒØ§Ù†Øª ØªØ¬Ø±Ø¨Ø© Ø³ÙŠØ¦Ø© Ù…Ø¹ÙƒÙ… Ø§Ù…Ø³ ...    neg  \n",
       "2     RT @Mur_Hus: @Saad_Maestro @MaestroPizzaKSA @l...    neg  \n",
       "3     @SubwaySaudi Ø¢Ø®Ø± Ù…Ø±Ø© Ø¨Ø¥Ø°Ù† Ø§Ù„Ù„Ù‡ Ø§Ø´ØªØ±ÙŠ Ù…Ù†ÙƒÙ… ØŒ ÙƒÙ„...    neg  \n",
       "4                         Ø§Ù„ÙŠÙˆÙ… Ø¨Ø±ÙˆØ­ Ø³ÙƒØ´Ù† Ø¨ÙŠ Ù…Ù†Ù‚Ø¨Ø© Ø¹Ù†Ø§Ø¯    neg  \n",
       "...                                                 ...    ...  \n",
       "3738  Ø¬Ù…ÙŠÙ„ Ø§Ù„Ù…Ø·Ø¹Ù… Ø¨ÙŠ Ø§Ù„Ø²Ø­Ù…Ø© ØªÙƒØ±Ù‡Ù†ÙŠ ÙÙŠÙ‡ .. ÙˆØ§Ù„Ù„Ù‡ ØªØ¹Ù‚Ø¯...    neg  \n",
       "3739  Ø§Ù„Ø´Ø§ÙˆØ±Ù…Ø§ â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸Ø§Ù„ÙƒØ¨Ø§Ø¨ â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸Ø£ÙˆØµØ§Ù„ Ø§Ù„Ø¯Ø¬...    pos  \n",
       "3740  Ù„Ø§ØªÙˆØ¬Ø¯ Ù‚Ù‡ÙˆÙ‡ Ø¶Ù…Ù† Ø§Ù„Ø§ÙØ·Ø§Ø± ! ÙÙ‚Ø· Ø¹ØµÙŠØ±Ø§Øª Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø³...    neg  \n",
       "3741                           Ø§Ù„Ø´ÙˆØ±Ù…Ø§ Ùˆ ÙØ·ÙŠØ±Ø© Ø§Ù„Ø¬Ø¨Ù† ğŸ‘ŒğŸ¼    pos  \n",
       "3742  The #shawarma, juices and fatayer         Ù„Ø§ ÙŠ...    pos  \n",
       "\n",
       "[3743 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3743, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3743 entries, 0 to 3742\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   name    3743 non-null   object\n",
      " 1   text    3743 non-null   object\n",
      " 2   Stance  3743 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 87.9+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data[data['Stance']!='neu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1904 entries, 0 to 3742\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   name    1904 non-null   object\n",
      " 1   text    1904 non-null   object\n",
      " 2   Stance  1904 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 59.5+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ibrahim\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:5303: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "data.Stance=data.Stance.map({'neg':0,'pos':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>text</th>\n",
       "      <th>Stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ù…Ø´Ø§Ø¹Ù„ #Ø§Ù„Ø²Ø¹ÙŠÙ…_Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠ_Ø§Ù„Ù…Ù„ÙƒÙŠ</td>\n",
       "      <td>Ø¥Ø¹Ù„Ø§Ù† ÙˆØ§Ø·ÙŠ Ù…Ù† Ø³ÙƒØ´Ù†-Ø¨ÙŠ ğŸ˜ğŸ˜‘ğŸ˜ https://t.co/uKArk6lg5k</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abureem</td>\n",
       "      <td>@shrimpshacksa Ù„Ù„Ø£Ø³Ù ÙƒØ§Ù†Øª ØªØ¬Ø±Ø¨Ø© Ø³ÙŠØ¦Ø© Ù…Ø¹ÙƒÙ… Ø§Ù…Ø³ ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ÙBo riyad ... â™š</td>\n",
       "      <td>RT @Mur_Hus: @Saad_Maestro @MaestroPizzaKSA @l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ø¥Ø¨Ø±Ø§Ù‡ÙŠÙ… Ø§Ù„Ù…Ù„Ø­Ù…</td>\n",
       "      <td>@SubwaySaudi Ø¢Ø®Ø± Ù…Ø±Ø© Ø¨Ø¥Ø°Ù† Ø§Ù„Ù„Ù‡ Ø§Ø´ØªØ±ÙŠ Ù…Ù†ÙƒÙ… ØŒ ÙƒÙ„...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Different</td>\n",
       "      <td>Ø§Ù„ÙŠÙˆÙ… Ø¨Ø±ÙˆØ­ Ø³ÙƒØ´Ù† Ø¨ÙŠ Ù…Ù†Ù‚Ø¨Ø© Ø¹Ù†Ø§Ø¯</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3738</th>\n",
       "      <td>adel 22</td>\n",
       "      <td>Ø¬Ù…ÙŠÙ„ Ø§Ù„Ù…Ø·Ø¹Ù… Ø¨ÙŠ Ø§Ù„Ø²Ø­Ù…Ø© ØªÙƒØ±Ù‡Ù†ÙŠ ÙÙŠÙ‡ .. ÙˆØ§Ù„Ù„Ù‡ ØªØ¹Ù‚Ø¯...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3739</th>\n",
       "      <td>Fares Aljumaid</td>\n",
       "      <td>Ø§Ù„Ø´Ø§ÙˆØ±Ù…Ø§ â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸Ø§Ù„ÙƒØ¨Ø§Ø¨ â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸Ø£ÙˆØµØ§Ù„ Ø§Ù„Ø¯Ø¬...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3740</th>\n",
       "      <td>M</td>\n",
       "      <td>Ù„Ø§ØªÙˆØ¬Ø¯ Ù‚Ù‡ÙˆÙ‡ Ø¶Ù…Ù† Ø§Ù„Ø§ÙØ·Ø§Ø± ! ÙÙ‚Ø· Ø¹ØµÙŠØ±Ø§Øª Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø³...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3741</th>\n",
       "      <td>shahd mohamad</td>\n",
       "      <td>Ø§Ù„Ø´ÙˆØ±Ù…Ø§ Ùˆ ÙØ·ÙŠØ±Ø© Ø§Ù„Ø¬Ø¨Ù† ğŸ‘ŒğŸ¼</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3742</th>\n",
       "      <td>Rayyan Alwazzan</td>\n",
       "      <td>The #shawarma, juices and fatayer         Ù„Ø§ ÙŠ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1904 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              name  \\\n",
       "0     Ù…Ø´Ø§Ø¹Ù„ #Ø§Ù„Ø²Ø¹ÙŠÙ…_Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠ_Ø§Ù„Ù…Ù„ÙƒÙŠ   \n",
       "1                          Abureem   \n",
       "2                  ÙBo riyad ... â™š   \n",
       "3                   Ø¥Ø¨Ø±Ø§Ù‡ÙŠÙ… Ø§Ù„Ù…Ù„Ø­Ù…   \n",
       "4                        Different   \n",
       "...                            ...   \n",
       "3738                       adel 22   \n",
       "3739                Fares Aljumaid   \n",
       "3740                             M   \n",
       "3741                 shahd mohamad   \n",
       "3742               Rayyan Alwazzan   \n",
       "\n",
       "                                                   text  Stance  \n",
       "0     Ø¥Ø¹Ù„Ø§Ù† ÙˆØ§Ø·ÙŠ Ù…Ù† Ø³ÙƒØ´Ù†-Ø¨ÙŠ ğŸ˜ğŸ˜‘ğŸ˜ https://t.co/uKArk6lg5k       0  \n",
       "1     @shrimpshacksa Ù„Ù„Ø£Ø³Ù ÙƒØ§Ù†Øª ØªØ¬Ø±Ø¨Ø© Ø³ÙŠØ¦Ø© Ù…Ø¹ÙƒÙ… Ø§Ù…Ø³ ...       0  \n",
       "2     RT @Mur_Hus: @Saad_Maestro @MaestroPizzaKSA @l...       0  \n",
       "3     @SubwaySaudi Ø¢Ø®Ø± Ù…Ø±Ø© Ø¨Ø¥Ø°Ù† Ø§Ù„Ù„Ù‡ Ø§Ø´ØªØ±ÙŠ Ù…Ù†ÙƒÙ… ØŒ ÙƒÙ„...       0  \n",
       "4                         Ø§Ù„ÙŠÙˆÙ… Ø¨Ø±ÙˆØ­ Ø³ÙƒØ´Ù† Ø¨ÙŠ Ù…Ù†Ù‚Ø¨Ø© Ø¹Ù†Ø§Ø¯       0  \n",
       "...                                                 ...     ...  \n",
       "3738  Ø¬Ù…ÙŠÙ„ Ø§Ù„Ù…Ø·Ø¹Ù… Ø¨ÙŠ Ø§Ù„Ø²Ø­Ù…Ø© ØªÙƒØ±Ù‡Ù†ÙŠ ÙÙŠÙ‡ .. ÙˆØ§Ù„Ù„Ù‡ ØªØ¹Ù‚Ø¯...       0  \n",
       "3739  Ø§Ù„Ø´Ø§ÙˆØ±Ù…Ø§ â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸Ø§Ù„ÙƒØ¨Ø§Ø¨ â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸Ø£ÙˆØµØ§Ù„ Ø§Ù„Ø¯Ø¬...       1  \n",
       "3740  Ù„Ø§ØªÙˆØ¬Ø¯ Ù‚Ù‡ÙˆÙ‡ Ø¶Ù…Ù† Ø§Ù„Ø§ÙØ·Ø§Ø± ! ÙÙ‚Ø· Ø¹ØµÙŠØ±Ø§Øª Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø³...       0  \n",
       "3741                           Ø§Ù„Ø´ÙˆØ±Ù…Ø§ Ùˆ ÙØ·ÙŠØ±Ø© Ø§Ù„Ø¬Ø¨Ù† ğŸ‘ŒğŸ¼       1  \n",
       "3742  The #shawarma, juices and fatayer         Ù„Ø§ ÙŠ...       1  \n",
       "\n",
       "[1904 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in data.cla:\n",
    "#     print(i)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Stance.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1904, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    952\n",
       "0    952\n",
       "Name: Stance, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Stance.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = []\n",
    "neg = []\n",
    "for l in data.Stance:\n",
    "\n",
    "    if l == 1:\n",
    "        pos.append(1)\n",
    "        neg.append(0)\n",
    "        \n",
    "    elif l == 0: \n",
    "        pos.append(0)\n",
    "        neg.append(1)\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1904"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ibrahim\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\ibrahim\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "data['Pos']= pos\n",
    "data['Neg']= neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>text</th>\n",
       "      <th>Stance</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ù…Ø´Ø§Ø¹Ù„ #Ø§Ù„Ø²Ø¹ÙŠÙ…_Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠ_Ø§Ù„Ù…Ù„ÙƒÙŠ</td>\n",
       "      <td>Ø¥Ø¹Ù„Ø§Ù† ÙˆØ§Ø·ÙŠ Ù…Ù† Ø³ÙƒØ´Ù†-Ø¨ÙŠ ğŸ˜ğŸ˜‘ğŸ˜ https://t.co/uKArk6lg5k</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abureem</td>\n",
       "      <td>@shrimpshacksa Ù„Ù„Ø£Ø³Ù ÙƒØ§Ù†Øª ØªØ¬Ø±Ø¨Ø© Ø³ÙŠØ¦Ø© Ù…Ø¹ÙƒÙ… Ø§Ù…Ø³ ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ÙBo riyad ... â™š</td>\n",
       "      <td>RT @Mur_Hus: @Saad_Maestro @MaestroPizzaKSA @l...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ø¥Ø¨Ø±Ø§Ù‡ÙŠÙ… Ø§Ù„Ù…Ù„Ø­Ù…</td>\n",
       "      <td>@SubwaySaudi Ø¢Ø®Ø± Ù…Ø±Ø© Ø¨Ø¥Ø°Ù† Ø§Ù„Ù„Ù‡ Ø§Ø´ØªØ±ÙŠ Ù…Ù†ÙƒÙ… ØŒ ÙƒÙ„...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Different</td>\n",
       "      <td>Ø§Ù„ÙŠÙˆÙ… Ø¨Ø±ÙˆØ­ Ø³ÙƒØ´Ù† Ø¨ÙŠ Ù…Ù†Ù‚Ø¨Ø© Ø¹Ù†Ø§Ø¯</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           name  \\\n",
       "0  Ù…Ø´Ø§Ø¹Ù„ #Ø§Ù„Ø²Ø¹ÙŠÙ…_Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠ_Ø§Ù„Ù…Ù„ÙƒÙŠ   \n",
       "1                       Abureem   \n",
       "2               ÙBo riyad ... â™š   \n",
       "3                Ø¥Ø¨Ø±Ø§Ù‡ÙŠÙ… Ø§Ù„Ù…Ù„Ø­Ù…   \n",
       "4                     Different   \n",
       "\n",
       "                                                text  Stance  Pos  Neg  \n",
       "0  Ø¥Ø¹Ù„Ø§Ù† ÙˆØ§Ø·ÙŠ Ù…Ù† Ø³ÙƒØ´Ù†-Ø¨ÙŠ ğŸ˜ğŸ˜‘ğŸ˜ https://t.co/uKArk6lg5k       0    0    1  \n",
       "1  @shrimpshacksa Ù„Ù„Ø£Ø³Ù ÙƒØ§Ù†Øª ØªØ¬Ø±Ø¨Ø© Ø³ÙŠØ¦Ø© Ù…Ø¹ÙƒÙ… Ø§Ù…Ø³ ...       0    0    1  \n",
       "2  RT @Mur_Hus: @Saad_Maestro @MaestroPizzaKSA @l...       0    0    1  \n",
       "3  @SubwaySaudi Ø¢Ø®Ø± Ù…Ø±Ø© Ø¨Ø¥Ø°Ù† Ø§Ù„Ù„Ù‡ Ø§Ø´ØªØ±ÙŠ Ù…Ù†ÙƒÙ… ØŒ ÙƒÙ„...       0    0    1  \n",
       "4                      Ø§Ù„ÙŠÙˆÙ… Ø¨Ø±ÙˆØ­ Ø³ÙƒØ´Ù† Ø¨ÙŠ Ù…Ù†Ù‚Ø¨Ø© Ø¹Ù†Ø§Ø¯       0    0    1  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ibrahim\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "def remove_punct(text):\n",
    "    text_nopunct = ''\n",
    "    text_nopunct = re.sub('['+string.punctuation+']', '', text)\n",
    "    return text_nopunct\n",
    "\n",
    "data['Text_Clean'] = data['text'].apply(lambda x: remove_punct(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, WordNetLemmatizer\n",
    "tokens = [word_tokenize(sen) for sen in data.Text_Clean] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_token(tokens): \n",
    "    return [w.lower() for w in tokens]    \n",
    "    \n",
    "lower_tokens = [lower_token(token) for token in tokens] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stoplist = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(tokens): \n",
    "    return [word for word in tokens if word not in stoplist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_words = [remove_stop_words(sen) for sen in lower_tokens] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [' '.join(sen) for sen in filtered_words] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ibrahim\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "data['Text_Final'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ibrahim\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "data['tokens'] = filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['Text_Final', 'tokens', 'Stance', 'Pos', 'Neg']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text_Final</th>\n",
       "      <th>tokens</th>\n",
       "      <th>Stance</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ø¥Ø¹Ù„Ø§Ù† ÙˆØ§Ø·ÙŠ Ù…Ù† Ø³ÙƒØ´Ù†Ø¨ÙŠ ğŸ˜ğŸ˜‘ğŸ˜ httpstcoukark6lg5k</td>\n",
       "      <td>[Ø¥Ø¹Ù„Ø§Ù†, ÙˆØ§Ø·ÙŠ, Ù…Ù†, Ø³ÙƒØ´Ù†Ø¨ÙŠ, ğŸ˜ğŸ˜‘ğŸ˜, httpstcoukark6l...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>shrimpshacksa Ù„Ù„Ø£Ø³Ù ÙƒØ§Ù†Øª ØªØ¬Ø±Ø¨Ø© Ø³ÙŠØ¦Ø© Ù…Ø¹ÙƒÙ… Ø§Ù…Ø³ Ùˆ...</td>\n",
       "      <td>[shrimpshacksa, Ù„Ù„Ø£Ø³Ù, ÙƒØ§Ù†Øª, ØªØ¬Ø±Ø¨Ø©, Ø³ÙŠØ¦Ø©, Ù…Ø¹ÙƒÙ…...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rt murhus saadmaestro maestropizzaksa lcsaudi ...</td>\n",
       "      <td>[rt, murhus, saadmaestro, maestropizzaksa, lcs...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>subwaysaudi Ø¢Ø®Ø± Ù…Ø±Ø© Ø¨Ø¥Ø°Ù† Ø§Ù„Ù„Ù‡ Ø§Ø´ØªØ±ÙŠ Ù…Ù†ÙƒÙ… ØŒ ÙƒÙ„ ...</td>\n",
       "      <td>[subwaysaudi, Ø¢Ø®Ø±, Ù…Ø±Ø©, Ø¨Ø¥Ø°Ù†, Ø§Ù„Ù„Ù‡, Ø§Ø´ØªØ±ÙŠ, Ù…Ù†Ùƒ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Text_Final  \\\n",
       "0        Ø¥Ø¹Ù„Ø§Ù† ÙˆØ§Ø·ÙŠ Ù…Ù† Ø³ÙƒØ´Ù†Ø¨ÙŠ ğŸ˜ğŸ˜‘ğŸ˜ httpstcoukark6lg5k   \n",
       "1  shrimpshacksa Ù„Ù„Ø£Ø³Ù ÙƒØ§Ù†Øª ØªØ¬Ø±Ø¨Ø© Ø³ÙŠØ¦Ø© Ù…Ø¹ÙƒÙ… Ø§Ù…Ø³ Ùˆ...   \n",
       "2  rt murhus saadmaestro maestropizzaksa lcsaudi ...   \n",
       "3  subwaysaudi Ø¢Ø®Ø± Ù…Ø±Ø© Ø¨Ø¥Ø°Ù† Ø§Ù„Ù„Ù‡ Ø§Ø´ØªØ±ÙŠ Ù…Ù†ÙƒÙ… ØŒ ÙƒÙ„ ...   \n",
       "\n",
       "                                              tokens  Stance  Pos  Neg  \n",
       "0  [Ø¥Ø¹Ù„Ø§Ù†, ÙˆØ§Ø·ÙŠ, Ù…Ù†, Ø³ÙƒØ´Ù†Ø¨ÙŠ, ğŸ˜ğŸ˜‘ğŸ˜, httpstcoukark6l...       0    0    1  \n",
       "1  [shrimpshacksa, Ù„Ù„Ø£Ø³Ù, ÙƒØ§Ù†Øª, ØªØ¬Ø±Ø¨Ø©, Ø³ÙŠØ¦Ø©, Ù…Ø¹ÙƒÙ…...       0    0    1  \n",
       "2  [rt, murhus, saadmaestro, maestropizzaksa, lcs...       0    0    1  \n",
       "3  [subwaysaudi, Ø¢Ø®Ø±, Ù…Ø±Ø©, Ø¨Ø¥Ø°Ù†, Ø§Ù„Ù„Ù‡, Ø§Ø´ØªØ±ÙŠ, Ù…Ù†Ùƒ...       0    0    1  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = train_test_split(data, test_size=0.50, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1904 entries, 0 to 3742\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Text_Final  1904 non-null   object\n",
      " 1   tokens      1904 non-null   object\n",
      " 2   Stance      1904 non-null   int64 \n",
      " 3   Pos         1904 non-null   int64 \n",
      " 4   Neg         1904 non-null   int64 \n",
      "dtypes: int64(3), object(2)\n",
      "memory usage: 89.2+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "952"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10229 words total, with a vocabulary size of 3821\n",
      "Max sentence length is 37\n"
     ]
    }
   ],
   "source": [
    "all_training_words = [word for tokens in data_train[\"tokens\"] for word in tokens]\n",
    "training_sentence_lengths = [len(tokens) for tokens in data_train[\"tokens\"]]\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(training_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10064 words total, with a vocabulary size of 3791\n",
      "Max sentence length is 29\n"
     ]
    }
   ],
   "source": [
    "all_test_words = [word for tokens in data_test[\"tokens\"] for word in tokens]\n",
    "test_sentence_lengths = [len(tokens) for tokens in data_test[\"tokens\"]]\n",
    "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(test_sentence_lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Google News Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_path = r'Word2Vec_Model.bin'\n",
    "word2vec = models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
    "    if len(tokens_list)<1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "def get_word2vec_embeddings(vectors, clean_comments, generate_missing=False):\n",
    "    embeddings = clean_comments['tokens'].apply(lambda x: get_average_word2vec(x, vectors, \n",
    "                                                                                generate_missing=generate_missing))\n",
    "    return list(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_embeddings = get_word2vec_embeddings(word2vec, data_train, generate_missing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 50\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and Pad sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3821 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(data_train[\"Text_Final\"].tolist())\n",
    "training_sequences = tokenizer.texts_to_sequences(data_train[\"Text_Final\"].tolist())\n",
    "\n",
    "train_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(train_word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3822, 300)\n"
     ]
    }
   ],
   "source": [
    "train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
    "for word,index in train_word_index.items():\n",
    "    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "print(train_embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(data_test[\"Text_Final\"].tolist())\n",
    "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n",
    "    \n",
    "    #The embeddings matrix is passed to embedding_layer\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=False)\n",
    "    \n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    convs = []\n",
    "    filter_sizes = [2,3,4,5,6]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=200, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "        #Aggregation 5 filter into one value\n",
    "        l_pool = GlobalMaxPooling1D()(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "\n",
    "    l_merge = concatenate(convs, axis=1)\n",
    "\n",
    "    x = Dropout(0.1)(l_merge)  \n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    preds = Dense(labels_index, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = ['Pos', 'Neg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = data_train[label_names].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_cnn_data\n",
    "y_tr = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 50, 300)      1146600     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 49, 200)      120200      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 48, 200)      180200      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 47, 200)      240200      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 46, 200)      300200      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 45, 200)      360200      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 200)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 200)          0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 200)          0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 200)          0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 200)          0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1000)         0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1000)         0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          128128      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2)            258         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,475,986\n",
      "Trainable params: 1,329,386\n",
      "Non-trainable params: 1,146,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, \n",
    "                len(list(label_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 856 samples, validate on 96 samples\n",
      "Epoch 1/5\n",
      "856/856 [==============================] - 11s 13ms/step - loss: 0.6270 - acc: 0.7231 - val_loss: 0.5427 - val_acc: 0.7448\n",
      "Epoch 2/5\n",
      "856/856 [==============================] - 11s 13ms/step - loss: 0.4554 - acc: 0.8201 - val_loss: 0.4418 - val_acc: 0.8073\n",
      "Epoch 3/5\n",
      "856/856 [==============================] - 11s 13ms/step - loss: 0.3295 - acc: 0.8610 - val_loss: 0.4227 - val_acc: 0.8333\n",
      "Epoch 4/5\n",
      "856/856 [==============================] - 11s 13ms/step - loss: 0.2754 - acc: 0.8820 - val_loss: 0.5825 - val_acc: 0.7969\n",
      "Epoch 5/5\n",
      "856/856 [==============================] - 11s 13ms/step - loss: 0.1997 - acc: 0.9264 - val_loss: 0.6804 - val_acc: 0.8177\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train, y_tr, epochs=num_epochs, validation_split=0.1, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "952/952 [==============================] - 1s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_cnn_data, batch_size=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_labels=[]\n",
    "for p in predictions:\n",
    "    prediction_labels.append(labels[np.argmax(p)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8308823529411765"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(data_test.Stance==prediction_labels)/len(prediction_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    478\n",
       "1    474\n",
       "Name: Stance, dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.Stance.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
